GENERAL:
  name: IPAD
  seed: 47
  rgb: False
  resolution: &resolution [ 256, 256 ]
  max_epochs: 600
  data:
    pretrain:
      - /data/dataset/IrisCSD
      - /data/dataset/ND_CLD_2015
      - /data/dataset/ND_Contact_2010
    train:
      SelfTraining:
        config:
          skip: False
          selftraing: True
        paths:
          - /data/dataset/LivDet2017/Clarkson/train
          - /data/dataset/LivDet2017/IIIT_WVU/train
          - /data/dataset/LivDet2017/NotreDame/train
          - /data/dataset/LivDet2017/Clarkson/test
          - /data/dataset/LivDet2017/IIIT_WVU/test
          - /data/dataset/LivDet2017/NotreDame/test
      IIIT_WVU:
        config:
          skip: False
          selftraing: False
        paths:
          - /data/dataset/LivDet2017/IIIT_WVU/train
    test:
      Clarkson:
        config:
          gan: True
        paths:
          - /data/dataset/LivDet2017/Clarkson/test
      IIIT_WVU:
        config:
          gan: True
        paths:
          - /data/dataset/LivDet2017/IIIT_WVU/test
      NotreDame:
        config:
          gan: True
        paths:
          - /data/dataset/LivDet2017/NotreDame/test

CLASSIFIER:
  confidence_selfTraining: 0.9
  confidence_CUT: 0.8
  refresh_selftraining: 10
  batch_size: 32
  warmup: 30
  pad_token_id: -1
  self_training: True
  pretrain:
    batch_size: 8
    apply: True
    epochs: 5
    temperature: 0.07
    config:
      num_crops: [ 2, 4 ]
      crop_sizes: [ 224, 96 ]
      min_scale: [ 0.15, 0.05 ]
      max_scale: [ 1.0,  0.4 ]
      distortion_strength: 1.0
      sigma_range: [ 0.1, 2.0 ]
      gaussian_std: [ 0.05, 0.02 ]
    scheduler:
      type: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
      params:
        T_0: 250
        T_mult: 2
  model:
    type: torchvision.models.efficientnet_b0
    params:
      weights: null
    replacements:
      - name: "features.0.0"
        type: torch.nn.Conv2d
        params:
          in_channels: 1
          out_channels: 32
          kernel_size: 3
          stride: 2
          padding: 1
          bias: False
      - name: "classifier.1"
        type: torch.nn.Linear
        params:
          in_features: 1280
          out_features: 2
          bias: True
    extractor:
      output: classifier
      features:
        - features.7
        - features.6
        - features.5
        - features.4
  optimizer:
    type: monai.optimizers.Novograd
    params:
      amsgrad: True
      lr: 3.0e-3
      weight_decay: 1.0e-3
  scheduler:
    type: transformers.get_cosine_with_hard_restarts_schedule_with_warmup
    params:
      num_cycles: 10
      num_warmup_steps: -1
      num_training_steps: -1
  net_init:
    init_type: xavier
    init_gain: 0.02

CUT:
  flip_prob: 0.1
  batch_size: 32
  apply: False
  iterative: True
  update_freq: 5
  warmup: 30
  flip_equivariance: False
  lambda_GAN: 1.0
  lambda_NCE: 5.0
  nce_layers: [ 0,4,8,12,16 ]
  nce_T: 0.07
  nce_idt: True
  mode: lsgan
  netG:
    params:
      ngf: 128
      norm: batch
      use_dropout: False
      n_blocks: 9
      padding_type: reflect
      no_antialias: False
      no_antialias_up: False
    optimizer:
      type: torch.optim.Adam
      params:
        lr: 5.0e-4
        betas: [ 0.75, 0.999 ]
        weight_decay: 1.0e-3
    scheduler:
      type: transformers.get_cosine_with_hard_restarts_schedule_with_warmup
      params:
        num_cycles: 10
        num_warmup_steps: -1
        num_training_steps: -1
    net_init:
      init_type: normal
      init_gain: 0.02
  netD:
    params:
      ndf: 128
      n_layers: 5
      no_antialias: False
      norm: batch
    optimizer:
      type: torch.optim.Adam
      params:
        lr: 5.0e-4
        betas: [ 0.75, 0.999 ]
        weight_decay: 1.0e-3
    scheduler:
      type: transformers.get_cosine_with_hard_restarts_schedule_with_warmup
      params:
        num_cycles: 10
        num_warmup_steps: -1
        num_training_steps: -1
    net_init:
      init_type: normal
      init_gain: 0.02
  netF:
    params:
      use_mlp: True
      nc: 256
      num_patches: 128
    optimizer:
      type: torch.optim.Adam
      params:
        lr: 1.0e-3
        betas: [ 0.75, 0.999 ]
        weight_decay: 1.0e-3
    scheduler:
      type: transformers.get_cosine_with_hard_restarts_schedule_with_warmup
      params:
        num_cycles: 10
        num_warmup_steps: -1
        num_training_steps: -1
    net_init:
      init_type: normal
      init_gain: 0.02

AUGMENTATION:
  RandomHorizontalFlip:
    origin: torchvision.transforms
    params:
      p: 0.5
  RandomVerticalFlip:
    origin: torchvision.transforms
    params:
      p: 0.5
  RandomResizedCrop:
    origin: torchvision.transforms
    params:
      size: *resolution
      scale: !!python/tuple [ 0.6, 0.95 ]



