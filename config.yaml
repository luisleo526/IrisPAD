GENERAL:
  name: IPAD
  seed: 47
  rgb: False
  resolution: &resolution [ 256, 256 ]
  max_epochs: 1000
  pad_token_id: -1
  data:
    pretrain:
      config:
        num_crops: [ 2, 4 ]
        crop_sizes: [ 224, 96 ]
        min_scale: [ 0.14, 0.05 ]
        max_scale: [ 1, 0.14 ]
        distortion_strength: 1.0
        sigma_range: [ 0.1, 2.0 ]
      paths:
        - /data/dataset/IrisCSD
        - /data/dataset/ND_CLD_2015
        - /data/dataset/ND_Contact_2010
    train:
      SelfTraining:
        config:
          skip: False
          selftraing: True
        paths:
          - /data/dataset/LivDet2017/Clarkson/train
#          - /data/dataset/LivDet2017/IIIT_WVU/train
          - /data/dataset/LivDet2017/NotreDame/train
          - /data/dataset/LivDet2017/Clarkson/test
          - /data/dataset/LivDet2017/IIIT_WVU/test
          - /data/dataset/LivDet2017/NotreDame/test
      IIIT_WVU:
        config:
          skip: False
          selftraing: False
        paths:
          - /data/dataset/LivDet2017/IIIT_WVU/train
    test:
      Clarkson:
        config:
          gan: True
        paths:
          - /data/dataset/LivDet2017/Clarkson/test
      IIIT_WVU:
        config:
          gan: True
        paths:
          - /data/dataset/LivDet2017/IIIT_WVU/test
      NotreDame:
        config:
          gan: True
        paths:
          - /data/dataset/LivDet2017/NotreDame/test

CLASSIFIER:
  self_training: False
  confidence_selfTraining: 0.9
  confidence_CUT: 0.75
  refresh_selftraining: 10
  batch_size: 32
  warmup: 30
  model:
    type: torchvision.models.efficientnet_b0
    params:
      weights: DEFAULT
    replacements:
      - name: "features.0.0"
        type: torch.nn.Conv2d
        params:
          in_channels: 1
          out_channels: 32
          kernel_size: 3
          stride: 2
          padding: 1
          bias: False
      - name: "classifier.1"
        type: torch.nn.Linear
        params:
          in_features: 1280
          out_features: 2
          bias: True
  optimizer:
    type: monai.optimizers.Novograd
    params:
      amsgrad: True
      lr: 3.0e-3
      weight_decay: 1.0e-3
  scheduler:
    type: transformers.get_cosine_with_hard_restarts_schedule_with_warmup
    params:
      num_cycles: 10
      num_warmup_steps: &warm_steps 5000
      num_training_steps: -1
  net_init:
    init_type: xavier
    init_gain: 0.02

CUT:
  flip_prob: 0.2
  batch_size: 16
  apply: True
  iterative: True
  update_freq: 5
  warmup: 20
  flip_equivariance: False
  lambda_GAN: 1.0
  lambda_NCE: 1.0
  nce_layers: [ 0,4,8,12,16,20 ]
  nce_T: 0.07
  nce_idt: True
  mode: lsgan
  netG:
    params:
      ngf: 64
      norm: batch
      use_dropout: False
      n_blocks: 6
      padding_type: reflect
      no_antialias: False
      no_antialias_up: False
    optimizer:
      type: monai.optimizers.Novograd
      params:
        lr: 5.0e-4
        betas: [ 0.75, 0.999 ]
        weight_decay: 1.0e-3
    scheduler:
      type: torch.optim.lr_scheduler.StepLR
      params:
        step_size: 30
        gamma: 0.5
    net_init:
      init_type: normal
      init_gain: 0.02
  netD:
    params:
      ndf: 64
      n_layers: 5
      no_antialias: False
      norm: batch
    optimizer:
      type: monai.optimizers.Novograd
      params:
        lr: 5.0e-4
        betas: [ 0.75, 0.999 ]
        weight_decay: 1.0e-3
    scheduler:
      type: torch.optim.lr_scheduler.StepLR
      params:
        step_size: 30
        gamma: 0.5
    net_init:
      init_type: xavier
      init_gain: 0.02
  netF:
    params:
      use_mlp: True
      nc: 256
      num_patches: 128
    optimizer:
      type: monai.optimizers.Novograd
      params:
        lr: 1.0e-3
        betas: [ 0.75, 0.999 ]
        weight_decay: 1.0e-3
    scheduler:
      type: torch.optim.lr_scheduler.StepLR
      params:
        step_size: 30
        gamma: 0.5
    net_init:
      init_type: normal
      init_gain: 0.02

AUGMENTATION:
  RandomHorizontalFlip:
    origin: torchvision.transforms
    params:
      p: 0.5
  RandomVerticalFlip:
    origin: torchvision.transforms
    params:
      p: 0.5
  RandomResizedCrop:
    origin: torchvision.transforms
    params:
      size: *resolution
      scale: !!python/tuple [ 0.6, 0.95 ]



