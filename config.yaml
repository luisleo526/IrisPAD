GENERAL:
  name: IPAD
  seed: 47
  rgb: False
  resolution: &resolution [ 256, 256 ]
  max_epochs: 300
  data:
    pretrain:
      - /data/dataset/IrisCSD
      - /data/dataset/ND_CLD_2015
      - /data/dataset/ND_Contact_2010
      - /data/dataset/LivDet2017
    train:
      SelfTraining:
        config:
          skip: False
          selftraining: True
        paths:
          # - /data/dataset/LivDet2017/Clarkson/train
          - /data/dataset/LivDet2017/Clarkson/test
          - /data/dataset/LivDet2017/IIIT_WVU/train
          - /data/dataset/LivDet2017/IIIT_WVU/test
          - /data/dataset/LivDet2017/NotreDame/train
          - /data/dataset/LivDet2017/NotreDame/test
      NotreDame:
        config:
          skip: False
          selftraining: False
        paths:
          - /data/dataset/LivDet2017/NotreDame/train
    test:
      Clarkson:
        config:
          gan: True
        paths:
          - /data/dataset/LivDet2017/Clarkson/test
      IIIT_WVU:
        config:
          gan: True
        paths:
          - /data/dataset/LivDet2017/IIIT_WVU/test
      NotreDame:
        config:
          gan: True
        paths:
          - /data/dataset/LivDet2017/NotreDame/test

CLASSIFIER:
  confidence_selfTraining: 0.9
  confidence_CUT: 0.8
  refresh_selftraining: 10
  batch_size: 32
  warmup: 30
  pad_token_id: -1
  self_training: False
  pretrain:
    batch_size: 8
    apply: False
    epochs: 5
    temperature: 0.07
    config:
      num_crops: [ 2, 4 ]
      crop_sizes: [ 224, 96 ]
      min_scale: [ 0.15, 0.05 ]
      max_scale: [ 1.0,  0.2 ]
      distortion_strength: 1.0
      sigma_range: [ 0.1, 2.0 ]
      gaussian_std: [ 0.1, 0.05 ]
    optimizer:
      type: torch.optim.Adam
      params:
        lr: 1.0e-2
    scheduler:
      type: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
      params:
        T_0: 250
        T_mult: 2
  model:
    type: torchvision.models.efficientnet_b0
    params:
      weights: DEFAULT
    replacements:
      - name: "features.0.0"
        type: torch.nn.Conv2d
        params:
          in_channels: 1
          out_channels: 32
          kernel_size: 3
          stride: 2
          padding: 1
          bias: False
      - name: "classifier.1"
        type: torch.nn.Linear
        params:
          in_features: 1280
          out_features: 2
          bias: True
    extractor:
      features:
        - features.7
        - features.5
        - features.3
  optimizer:
    type: torch.optim.Adam
    params:
      lr: 5.0e-3
      weight_decay: 1.0e-3
    group:
      -
        params: features
        lr: 5.0e-3
        weight_decay: 1.0e-3
      -
        params: classifier
        lr: 5.0e-4
  scheduler:
    type: torch.optim.lr_scheduler.ReduceLROnPlateau
    params:
      mode: "min"
      factor: 0.67
      patience: 10
      cooldown: 3
      min_lr: 1.0e-8
  net_init:
    init_type: xavier
    init_gain: 0.02

CUT:
  batch_size: 16
  apply: True
  iterative: True
  update_freq: 5
  warmup: 30
  flip_equivariance: False
  lambda_GAN: 1.0
  lambda_NCE: 0.5
  nce_layers: [ 0,4,8,12,16,20 ]
  nce_T: 0.07
  nce_idt: True
  mode: lsgan
  netG:
    params:
      ngf: 64
      norm: batch
      use_dropout: True
      n_blocks: 9
      padding_type: reflect
      no_antialias: False
      no_antialias_up: False
    optimizer:
      type: torch.optim.Adam
      params:
        lr: 5.0e-5
        betas: [ 0.75, 0.999 ]
        weight_decay: 1.0e-3
    scheduler:
      type: torch.optim.lr_scheduler.ReduceLROnPlateau
      params:
        mode: "min"
        factor: 0.5
        patience: 5
        cooldown: 2
        min_lr: 1.0e-5
    net_init:
      init_type: normal
      init_gain: 0.02
  netD:
    params:
      ndf: 64
      n_layers: 3
      no_antialias: False
      norm: batch
    optimizer:
      type: torch.optim.Adam
      params:
        lr: 5.0e-5
        betas: [ 0.75, 0.999 ]
        weight_decay: 1.0e-3
    scheduler:
      type: torch.optim.lr_scheduler.ReduceLROnPlateau
      params:
        mode: "min"
        factor: 0.5
        patience: 5
        cooldown: 2
        min_lr: 1.0e-5
    net_init:
      init_type: normal
      init_gain: 0.02
  netF:
    params:
      use_mlp: True
      nc: 256
      num_patches: 128
    optimizer:
      type: torch.optim.Adam
      params:
        lr: 5.0e-5
        betas: [ 0.999, 0.999 ]
        weight_decay: 1.0e-3
    scheduler:
      type: torch.optim.lr_scheduler.ReduceLROnPlateau
      params:
        mode: "min"
        factor: 0.5
        patience: 5
        cooldown: 2
        min_lr: 1.0e-5
    net_init:
      init_type: normal
      init_gain: 0.02

AUGMENTATION:
  RandomHorizontalFlip:
    origin: torchvision.transforms
    params:
      p: 0.5
  RandomVerticalFlip:
    origin: torchvision.transforms
    params:
      p: 0.5
  RandomResizedCrop:
    origin: torchvision.transforms
    params:
      size: *resolution
      scale: !!python/tuple [ 0.6, 0.95 ]



